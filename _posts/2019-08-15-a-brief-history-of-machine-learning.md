---
layout: post
title: 机器学习简史
---

---

# 一、早期神经网络阶段

## 1949年  赫布理论

普遍ML被Hebb（赫布）提出，基于神经心理学的构想；

## **1952年，Arthur Samuel   亚瑟 萨缪尔  机器学习之父**

在IBM的时候开发了一个玩跳棋的程序，该程序能够通过棋子的位置学习一个隐式模型，为下一步棋提供比较好的走法。 塞缪尔与这个程序对局了很多次，并观察到这个程序在经过一段时间的学习后可以发挥得更好。**他给机器学习下了一个定义：机器学习是一个能使计算机不用显示编程就能获得能力的研究领域。**

## 1957年，感知器模型被第二次提出

Rosenblatt的基于神经科学背景（基础）的感知器模型被第二次提出，并且取得了一定的发展

## 1969年，Minsky 提出异或问题，神经网络遇到瓶颈，

被Minsky 提出的异或困扰，神经网络 一下子沉寂了10年，到80年代才有所突破

## 1981年，神经网络特定反向传播（BP）算法应用到多层感知器（MLP）

直到1981年，Werbos [6]才提出将神经网络特定反向传播（BP）算法应用到多层感知器（MLP）。BP仍然是当今神经网络架构的关键组成部分。有了这些新想法，神经网络的研究再次加速。1985年至1986年间，神经网络研究人员相继提出了采用BP训练多层感知器（MLP）的理念

----

# 二、统计学习方法的春天

## 1986年，决策树   

另一个学派，在1986年，J.R.Quinlan 提出了另一个非常著名的ML算法，我们称之为**决策树**，更具体地说是ID3算法。这是机器学习另一个主流的闪光点。此外，ID3能够以简单的规则及其明确的推论更好地应用到实际生活中，与黑匣子神经网络模型相反。在ID3之后，出现了很多不同的可用方案和算法改进（例如ID4，回归树，CART …），而且仍然是机器学习中的活跃话题之一。

## 1995年， SVM

SVM具有非常强的理论论证和实证结果，在2000年左右，SVM内核化版本提出之后在很多之前用NN模型解决的问题上得出了最佳结果。 此外，与NN模型相比，SVM能够充分利用凸优化，泛化边际理论和内核化的所有深奥知识。 因此，它可以从不同学科中获得巨大的推动力，促进理论和实践的快速发展。

## 1997年，Adaboost

1997年提出了另一个实体机器学习模型，该模型采用增强的弱分类器组合，称为Adaboost，Adaboost通过易于训练的弱分类器进行训练，给那些难的样本更高的权重。这种模型是许多不同任务的基础，如面部识别和检测。

---

# 三、深度神经网络

## 2005年，深度学习崛起

NN 模型能够在对象识别、语音识别、NLP等不同的任务中击败之前的技术。但是应该注意的是，这并不意味着其他ML流派的结束。虽然深度学习的成功故事还在接二连三的上演，但是它在训练成本和调整模型的外部参数方面还有很多争议。此外，由于其简单性，SVM的使用依然非常普遍。  

## 2012年，CNN碾压SVM，从此统治AI界

Hinton课题组为了证明深度学习的潜力，首次参加ImageNet图像识别比赛，其通过构建的CNN网络**AlexNet**一举夺得冠军，且碾压第二名（SVM方法）的分类性能。也正是由于该比赛，CNN吸引到了众多研究者的注意。 

----

